{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the lab lesson_\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient tensor([3.7730, 1.8865, 0.0651, 0.0651, 0.0765])\n",
      "y tensor(4.0584, grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.,2., -1., 3., 5.], requires_grad=True)\n",
    "y = torch.log(torch.exp(x[0]*x[1])**2 + torch.sin(x[2]+x[3]+x[4])*x[4])\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"gradient\",x.grad)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical calculation of the gradient in the other attachment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MlpClass of the previous assignment\n",
    "class MyMLP(torch.nn.Module):\n",
    "    def __init__(self,bias = True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5,11,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11,16,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,13,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13,8,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8,4,bias=bias),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "n = 10\n",
    "x = torch.rand([n,5])\n",
    "y = one_hot(torch.torch.randint(4,[n]))\n",
    "\n",
    "model = MyMLP(bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(y_model : torch.tensor, y_true : torch.tensor):\n",
    "    assert y_model.shape[0] == y_true.shape[0], f\"Mismatched number of examples got: y_model -> {y_model.shape[0]} y_true -> {y_true.shape[0]} \"\n",
    "    assert y_model.shape[1] == y_true.shape[1], f\"Mismatched number of classes got: y_model -> {y_model.shape[1]} y_true -> {y_true.shape[1]} \"\n",
    "\n",
    "    return -torch.sum( y_true * torch.log(y_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2495, 0.2507, 0.2494, 0.2504],\n",
      "        [0.2499, 0.2501, 0.2500, 0.2501],\n",
      "        [0.2499, 0.2504, 0.2496, 0.2501],\n",
      "        [0.2496, 0.2504, 0.2498, 0.2502],\n",
      "        [0.2500, 0.2510, 0.2492, 0.2498],\n",
      "        [0.2502, 0.2527, 0.2473, 0.2498],\n",
      "        [0.2498, 0.2502, 0.2498, 0.2501],\n",
      "        [0.2498, 0.2503, 0.2497, 0.2502],\n",
      "        [0.2497, 0.2503, 0.2498, 0.2502],\n",
      "        [0.2495, 0.2507, 0.2494, 0.2504]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_model = model.forward(x)\n",
    "print(y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.872485160827637\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropyLoss(y_model,y)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward gradient calculation on the loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7753e-03, -3.5980e-03, -4.0910e-03, -5.8204e-03, -1.0078e-02],\n",
      "        [ 6.0736e-04, -3.6431e-03, -4.4799e-03, -5.6679e-03, -4.6795e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.2994e-03,  6.8594e-03,  7.1887e-03, -2.1809e-03,  3.4732e-03],\n",
      "        [-8.1990e-03, -1.5343e-02, -1.6551e-02, -4.9847e-03, -1.9007e-02],\n",
      "        [ 9.9195e-03,  2.0639e-02,  1.9778e-02,  1.8529e-03,  2.2566e-02],\n",
      "        [-5.3381e-04, -1.3853e-03, -2.4290e-03, -2.0391e-03, -8.6166e-04],\n",
      "        [ 1.6323e-03,  4.2106e-04,  1.7260e-04,  3.3338e-03,  1.0164e-03],\n",
      "        [ 9.8829e-03,  5.9990e-03,  6.7872e-03,  8.3505e-03,  1.4986e-02],\n",
      "        [-2.7566e-05,  5.2267e-03,  3.9838e-03, -6.2747e-03, -7.2611e-05],\n",
      "        [ 1.9258e-03, -2.7946e-03, -9.2548e-04,  3.4578e-04, -1.3324e-03]])\n"
     ]
    }
   ],
   "source": [
    "first_layer = list(model.parameters())[0]\n",
    "print(first_layer.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula given by the script, input `Francesco Tomba`\n",
    "\n",
    "$f(X) =  \\tan((\\text{ReLU}(x_1 * x_2) - \\text{atan}(x_3 - x_4)) / x_5)$\n",
    "\n",
    "Values given by the script: \n",
    "\n",
    "$\\vec{X} = [ -2, 2, 3, -2, 1]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First part in the attachment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.0000, grad_fn=<TanBackward0>)\n",
      "tensor([ 0.0000,  0.0000, -1.0000,  1.0000, 35.7084])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-2., 2., 3., -2., 1], requires_grad=True)\n",
    "y = torch.tan((x[0]*x[1]).relu() - (x[2] - x[3]).atan() / x[4])\n",
    "\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8feeccfccaf5a7e6e6ab46f8ad1c6cf5343db38cddefdd7c9d39ef2b4cb6dc36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
