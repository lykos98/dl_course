{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} y_i \\log(\\hat{y}_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the lab lesson_\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient tensor([3.7730, 1.8865, 0.0651, 0.0651, 0.0765])\n",
      "y tensor(4.0584, grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.,2., -1., 3., 5.], requires_grad=True)\n",
    "y = torch.log(torch.exp(x[0]*x[1])**2 + torch.sin(x[2]+x[3]+x[4])*x[4])\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"gradient\",x.grad)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical calculation of the gradient in the other attachment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MlpClass of the previous assignment\n",
    "class MyMLP(torch.nn.Module):\n",
    "    def __init__(self,bias = True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5,11,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11,16,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,13,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13,8,bias=bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8,4,bias=bias),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "n = 10\n",
    "x = torch.rand([n,5])\n",
    "# note, one hot expects labels from 0 to n-1 with n classes\n",
    "# actually a little trick for obtaining correct labels\n",
    "# y = one_hot(torch.tensor(labels), num_classes=4)\n",
    "# this command actually returns an error \n",
    "# i did not find a more elegant solution\n",
    "labels = torch.tensor([1,2,3,4,1,2,3,4,1,2]) - 1\n",
    "y = one_hot(labels)\n",
    "print(y)\n",
    "model = MyMLP(bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(y_model : torch.tensor, y_true : torch.tensor):\n",
    "    assert y_model.shape[0] == y_true.shape[0], f\"Mismatched number of examples got: y_model -> {y_model.shape[0]} y_true -> {y_true.shape[0]} \"\n",
    "    assert y_model.shape[1] == y_true.shape[1], f\"Mismatched number of classes got: y_model -> {y_model.shape[1]} y_true -> {y_true.shape[1]} \"\n",
    "\n",
    "    return -torch.sum( y_true * torch.log(y_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2496, 0.2492, 0.2528, 0.2484],\n",
      "        [0.2493, 0.2490, 0.2537, 0.2480],\n",
      "        [0.2498, 0.2496, 0.2513, 0.2493],\n",
      "        [0.2502, 0.2487, 0.2534, 0.2477],\n",
      "        [0.2493, 0.2490, 0.2535, 0.2482],\n",
      "        [0.2497, 0.2492, 0.2528, 0.2482],\n",
      "        [0.2502, 0.2491, 0.2524, 0.2483],\n",
      "        [0.2500, 0.2488, 0.2532, 0.2480],\n",
      "        [0.2496, 0.2491, 0.2524, 0.2489],\n",
      "        [0.2502, 0.2493, 0.2515, 0.2490]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_model = model.forward(x)\n",
    "print(y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.881470680236816\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropyLoss(y_model,y)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward gradient calculation on the loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#getting the first layer parameters\n",
    "first_layer = list(model.parameters())[0]\n",
    "print(first_layer.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula given by the script, input `Francesco Tomba`\n",
    "\n",
    "$f(X) =  \\tan((\\text{ReLU}(x_1 * x_2) - \\text{atan}(x_3 - x_4)) / x_5)$\n",
    "\n",
    "Values given by the script: \n",
    "\n",
    "$\\vec{X} = [ -2, 2, 3, -2, 1]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First part in the attachment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor(-5.0000, grad_fn=<TanBackward0>)\n",
      "gradient tensor([ 0.0000,  0.0000, -1.0000,  1.0000, 35.7084])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-2., 2., 3., -2., 1], requires_grad=True)\n",
    "y = torch.tan((x[0]*x[1]).relu() - (x[2] - x[3]).atan() / x[4])\n",
    "\n",
    "y.backward()\n",
    "print(\"y\",y)\n",
    "print(\"gradient\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8feeccfccaf5a7e6e6ab46f8ad1c6cf5343db38cddefdd7c9d39ef2b4cb6dc36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
